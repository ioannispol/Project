{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unavailable-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from skimage import io\n",
    "from PIL import Image\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Project Image dataset\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        #image = io.imread(img_path)\n",
    "        y_label = torch.tensor(int(self.annotations.iloc[index, 2]))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "strong-cambridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageDataset(\n",
    "    csv_file='../src/data/raw_labes.csv', \n",
    "    root_dir='../src/data/raw', \n",
    "    transform=transforms.ToTensor()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "virtual-panel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToTensor()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "blessed-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_file='../dataset/set_dataset/train_labes.csv'\n",
    "train_root_dir='../dataset/set_dataset/train/'\n",
    "\n",
    "annotations = pd.read_csv(train_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "realistic-cisco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>class</th>\n",
       "      <th>class_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bolt_0.jpg</td>\n",
       "      <td>bolt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bolt_1.jpg</td>\n",
       "      <td>bolt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bolt_10.jpg</td>\n",
       "      <td>bolt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bolt_102.jpg</td>\n",
       "      <td>bolt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bolt_103.jpg</td>\n",
       "      <td>bolt</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1514</th>\n",
       "      <td>pipe_444.jpg</td>\n",
       "      <td>pipe</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>pipe_445.jpg</td>\n",
       "      <td>pipe</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>pipe_446.jpg</td>\n",
       "      <td>pipe</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>pipe_447.jpg</td>\n",
       "      <td>pipe</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>pipe_448.jpg</td>\n",
       "      <td>pipe</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1519 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename class  class_num\n",
       "0       bolt_0.jpg  bolt          1\n",
       "1       bolt_1.jpg  bolt          1\n",
       "2      bolt_10.jpg  bolt          1\n",
       "3     bolt_102.jpg  bolt          1\n",
       "4     bolt_103.jpg  bolt          1\n",
       "...            ...   ...        ...\n",
       "1514  pipe_444.jpg  pipe          5\n",
       "1515  pipe_445.jpg  pipe          5\n",
       "1516  pipe_446.jpg  pipe          5\n",
       "1517  pipe_447.jpg  pipe          5\n",
       "1518  pipe_448.jpg  pipe          5\n",
       "\n",
       "[1519 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "played-wagon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1519"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "automated-desire",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'root_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3d319be8f704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'root_dir' is not defined"
     ]
    }
   ],
   "source": [
    "img_path = os.path.join(root_dir, annotations.iloc[0, 0])\n",
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "solar-usage",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-3511e04ac49a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_path' is not defined"
     ]
    }
   ],
   "source": [
    "image = io.imread(img_path)\n",
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "normal-possibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_label = torch.tensor(int(annotations.iloc[0, 2]))\n",
    "y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "breeding-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_data = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize([224,224]),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "tight-graduation",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-be14a7018bb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "image = transform_data(image)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "handed-vermont",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'img_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b280c24ef36a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'img_path' is not defined"
     ]
    }
   ],
   "source": [
    "image = Image.open(img_path)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "opposed-london",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../dataset/set_dataset/train'\n",
    "train_labels = '../dataset/set_dataset/train_labes.csv'\n",
    "val_path = '../dataset/set_dataset/val'\n",
    "val_labels = '../dataset/set_dataset/val_labes.csv'\n",
    "test_path = '../dataset/set_dataset/test'\n",
    "test_labels = '../dataset/set_dataset/test_labes.csv'\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "num_classes = 5\n",
    "learning_rate = 1e-3\n",
    "batch_size = 6\n",
    "num_epochs = 10\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))\n",
    "        ])\n",
    "\n",
    "test_val_transform = transforms.Compose([\n",
    "        transforms.Resize([224,224]),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))\n",
    "        ])\n",
    "\n",
    "train_dataset = ImageDataset(\n",
    "    csv_file=train_labels, \n",
    "    root_dir=train_path, \n",
    "    transform=train_transform\n",
    "    )\n",
    "val_dataset = ImageDataset(\n",
    "    csv_file=val_labels, \n",
    "    root_dir=val_path, \n",
    "    transform=test_val_transform\n",
    "    ) \n",
    "\n",
    "test_dataset = ImageDataset(\n",
    "    csv_file=test_labels, \n",
    "    root_dir=test_path, \n",
    "    transform=test_val_transform\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "vertical-reconstruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "banned-terrain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1524\n",
      "79\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(val_loader))\n",
    "print(len(test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "sharp-november",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1519\n",
      "474\n",
      "379\n",
      "2372\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(train_dataset) + len(val_dataset) + len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "developed-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "played-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_input_dims():\n",
    "        batch_data = torch.zeros((1, 3, 224, 224))\n",
    "        batch_data = conv1(batch_data)\n",
    "        batch_data = pool1(batch_data)\n",
    "        batch_data = conv2(batch_data)\n",
    "        batch_data = conv3(batch_data)\n",
    "        batch_data = pool2(batch_data)\n",
    "        batch_data = conv4(batch_data)\n",
    "        batch_data = pool3(batch_data)\n",
    "\n",
    "        return int(np.prod(batch_data.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "orange-video",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 5\n",
    "\n",
    "conv1 = nn.Conv2d(3, 64, 5)\n",
    "pool1 = nn.MaxPool2d(2, 2)\n",
    "conv2 = nn.Conv2d(64, 128, 5)\n",
    "conv3 = nn.Conv2d(128, 64, 5)\n",
    "pool2 = nn.MaxPool2d(2, 2)\n",
    "conv4 = nn.Conv2d(64, 64, 5)\n",
    "pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "input_dims = calc_input_dims()\n",
    "\n",
    "fc1 = nn.Linear(input_dims, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "naked-motor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33856"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "global-exclusion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=33856, out_features=5, bias=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "european-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 5)\n",
    "        self.conv3 = nn.Conv2d(128, 64, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 5)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        #input_dims = self.calc_input_dims()\n",
    "\n",
    "        self.fc1 = nn.Linear(224, self.num_classes)\n",
    "\n",
    "    # Function to calculate the input dimension to Linear layer\n",
    "    # TODO: Implement calculation depending on the network structure\n",
    "#     def calc_input_dims(self):\n",
    "#         batch_data = torch.zeros((1, 3, 224, 224))\n",
    "#         batch_data = self.conv1(batch_data)\n",
    "#         batch_data = self.pool1(batch_data)\n",
    "#         batch_data = self.conv2(batch_data)\n",
    "#         batch_data = self.conv3(batch_data)\n",
    "#         batch_data = self.pool2(batch_data)\n",
    "#         batch_data = self.conv4(batch_data)\n",
    "#         batch_data = self.pool3(batch_data)\n",
    "\n",
    "#         return int(np.prod(batch_data.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        print(x.shape)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accredited-atlas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=33856, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net().to(device)\n",
    "print(net)\n",
    "#summary(net, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "close-seventh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=44944, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=5, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 6, 220, 220]             456\n",
      "         MaxPool2d-2          [-1, 6, 110, 110]               0\n",
      "            Conv2d-3         [-1, 16, 106, 106]           2,416\n",
      "         MaxPool2d-4           [-1, 16, 53, 53]               0\n",
      "            Linear-5                  [-1, 120]       5,393,400\n",
      "            Linear-6                   [-1, 84]          10,164\n",
      "            Linear-7                    [-1, 5]             425\n",
      "================================================================\n",
      "Total params: 5,406,861\n",
      "Trainable params: 5,406,861\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 4.49\n",
      "Params size (MB): 20.63\n",
      "Estimated Total Size (MB): 25.69\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 53 * 53, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, 16 * 53 * 53)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net().to(device)\n",
    "print(net)\n",
    "summary(net, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "copyrighted-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data paths\n",
    "train_path = '../dataset/set_dataset/train'\n",
    "train_labels = '../dataset/set_dataset/train_labes.csv'\n",
    "val_path = '../dataset/set_dataset/val'\n",
    "val_labels = '../dataset/set_dataset/val_labes.csv'\n",
    "test_path = '../dataset/set_dataset/test'\n",
    "test_labels = '../dataset/set_dataset/test_labes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "failing-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_val = [0.5, 0.5, 0.5]\n",
    "std_val = [0.5, 0.5, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "radical-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_val, std_val),\n",
    "])\n",
    "\n",
    "test_val_transforms = transforms.Compose([\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_val, std_val),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "congressional-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ImageDataset(train_labels, train_path, train_transforms)\n",
    "val_data = ImageDataset(val_labels, val_path, test_val_transforms)\n",
    "test_data = ImageDataset(test_labels, test_path, test_val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "domestic-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=6, shuffle=True, num_workers=4)\n",
    "valloader = DataLoader(val_data, batch_size=6, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_data, batch_size=6, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "modified-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(train_data)\n",
    "val_size = len(val_data)\n",
    "test_size = len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "alert-barrier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 1519\n",
      "Validation data size: 474\n",
      "Test data size: 379\n",
      "Total data size: 2372\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train data size: {train_size}\\nValidation data size: {val_size}\\n\\\n",
    "Test data size: {test_size}\\nTotal data size: {train_size + val_size + test_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "capable-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "arctic-shell",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3ce786b6669b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "print('Finish Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-information",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 (torch-gpu)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
