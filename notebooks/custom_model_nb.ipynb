{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "absolute-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torchsummary import summary\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "grave-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "welcome-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Basic Dataset class which loads images from a folder\n",
    "\n",
    "Returns:\n",
    "    [type]: [description]\n",
    "\"\"\"\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Project Image dataset\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[index, 0])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        #image = Image.fromarray(image)\n",
    "        y_label = torch.tensor(int(self.annotations.iloc[index, 2]))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return (image, y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "round-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.conv1 = nn.Conv2d(3, 64, 5)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 5)\n",
    "        self.conv3 = nn.Conv2d(128, 64, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 5)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.fc1 = nn.Linear(64*23*23, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, 64 * 23 *23)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hindu-sampling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 220, 220]           4,864\n",
      "         MaxPool2d-2         [-1, 64, 110, 110]               0\n",
      "            Conv2d-3        [-1, 128, 106, 106]         204,928\n",
      "            Conv2d-4         [-1, 64, 102, 102]         204,864\n",
      "         MaxPool2d-5           [-1, 64, 51, 51]               0\n",
      "            Conv2d-6           [-1, 64, 47, 47]         102,464\n",
      "         MaxPool2d-7           [-1, 64, 23, 23]               0\n",
      "            Linear-8                    [-1, 6]         203,142\n",
      "================================================================\n",
      "Total params: 720,262\n",
      "Trainable params: 720,262\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 48.20\n",
      "Params size (MB): 2.75\n",
      "Estimated Total Size (MB): 51.52\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "downtown-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data paths\n",
    "train_path = '../dataset/set_dataset/train'\n",
    "train_labels = '../dataset/set_dataset/train_labes.csv'\n",
    "val_path = '../dataset/set_dataset/val'\n",
    "val_labels = '../dataset/set_dataset/val_labes.csv'\n",
    "test_path = '../dataset/set_dataset/test'\n",
    "test_labels = '../dataset/set_dataset/test_labes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "colored-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_val = [0.5, 0.5, 0.5]\n",
    "std_val = [0.5, 0.5, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "biological-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_val, std_val),\n",
    "])\n",
    "\n",
    "test_val_transforms = transforms.Compose([\n",
    "    transforms.Resize([224, 224]),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_val, std_val),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "greenhouse-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ImageDataset(train_labels, train_path, train_transforms)\n",
    "val_data = ImageDataset(val_labels, val_path, test_val_transforms)\n",
    "test_data = ImageDataset(test_labels, test_path, test_val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "exempt-idaho",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=6, shuffle=True, num_workers=4)\n",
    "valloader = DataLoader(val_data, batch_size=6, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_data, batch_size=6, shuffle=True, num_workers=4)\n",
    "\n",
    "classes = ('bolt', 'flange', 'lead_block', 'nut', 'pipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "behind-buyer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 1519\n",
      "Validation data size: 474\n",
      "Test data size: 379\n",
      "Total data size: 2372\n"
     ]
    }
   ],
   "source": [
    "train_size = len(train_data)\n",
    "val_size = len(val_data)\n",
    "test_size = len(test_data)\n",
    "\n",
    "print(f\"Train data size: {train_size}\\nValidation data size: {val_size}\\n\\\n",
    "Test data size: {test_size}\\nTotal data size: {train_size + val_size + test_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "owned-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "filled-stationery",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 1.727\n",
      "[1,    40] loss: 1.643\n",
      "[1,    60] loss: 1.655\n",
      "[1,    80] loss: 1.591\n",
      "[1,   100] loss: 1.533\n",
      "[1,   120] loss: 1.531\n",
      "[1,   140] loss: 1.432\n",
      "[1,   160] loss: 1.579\n",
      "[1,   180] loss: 1.445\n",
      "[1,   200] loss: 1.569\n",
      "[1,   220] loss: 1.539\n",
      "[1,   240] loss: 1.447\n",
      "[2,    20] loss: 1.450\n",
      "[2,    40] loss: 1.522\n",
      "[2,    60] loss: 1.433\n",
      "[2,    80] loss: 1.356\n",
      "[2,   100] loss: 1.414\n",
      "[2,   120] loss: 1.441\n",
      "[2,   140] loss: 1.431\n",
      "[2,   160] loss: 1.377\n",
      "[2,   180] loss: 1.359\n",
      "[2,   200] loss: 1.375\n",
      "[2,   220] loss: 1.469\n",
      "[2,   240] loss: 1.345\n",
      "Finish Training\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 20 == 0:    # print every 20 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 20))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "print('Finish Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "consecutive-monster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 379 test images: 44 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the {test_size} test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "complimentary-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_STORE_PATH = './'\n",
    "\n",
    "# Save the model and plot\n",
    "torch.save(model.state_dict(), MODEL_STORE_PATH + 'conv_net_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "distinguished-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show\n",
    "from bokeh.models import LinearAxis, Range1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "warming-mistress",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b6354568641c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_y_ranges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRange1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLinearAxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_range_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Accuracy (%)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_range_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_list' is not defined"
     ]
    }
   ],
   "source": [
    "p = figure(y_axis_label='Loss', width=850, y_range=(0, 1), title='PyTorch ConvNet results')\n",
    "p.extra_y_ranges = {'Accuracy': Range1d(start=0, end=100)}\n",
    "p.add_layout(LinearAxis(y_range_name='Accuracy', axis_label='Accuracy (%)'), 'right')\n",
    "p.line(np.arange(len(loss_list)), loss_list)\n",
    "p.line(np.arange(len(loss_list)), np.array(acc_list) * 100, y_range_name='Accuracy', color='red')\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-colonial",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 (torch-gpu)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
