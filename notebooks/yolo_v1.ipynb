{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vital-motel",
   "metadata": {},
   "source": [
    "# YOLO V1 implementation\n",
    "In this notebook the YOLOv1 will implemented based on the original [paper](https://arxiv.org/pdf/1506.02640v5.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-preserve",
   "metadata": {},
   "source": [
    "TODO: \n",
    "* [ ] Need to rewrite the plot function so to give the name and the probability prediction for each bounding box\n",
    "* [ ] Get more metrics from the training function (e.g. training and validation losses)\n",
    "* [ ] Write a function that will plot the training and validation loss as well as the training and validation accuracy\n",
    "* [ ] Use the model on the videos that I have from towing tank to see how well the algorithm performs\n",
    "* [ ] Use images/videos with darker light conditions to train and test the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-carol",
   "metadata": {},
   "source": [
    "# How Yolo works\n",
    "Yolo is an object detection algorithm and uses features that learned from a cnn network to detect objects. When prerforming object detection we want to correctly identify in the image the objects in the given image. Most of the classic aproaches in the object detection algorithms using the sliding window method where the classifier is run over evenly spaces lacations over the entire image. Such types of algorithms are the Deformable Parts Models (DPM), the R-CNN which uses proposal methods to generate the bounding boxes in the given image and then run the classifier on the proposed bounding boxes. This approch, and particullarly the DPM method is slow and not optimal for real time uses, and the improved version of R-CNN models is gaining some speed by strategically selecting interesting regions and run through them the classifier.\n",
    "\n",
    "On the other hand YOLO algorithm based on the idea to split the image in a grid, for axample for a given image we can split it in a 3 by 3 grid (**_SxS = 3x3_**) which gives as 9 cells. As the below image shows, the image consists by a 3 by 3 grid with 9 cells, and each cell has 2 bouning boxes (**_B_**) which finally will give the prediction bounding boxe for the object in the image.\n",
    "\n",
    "![image](../notes/images/image.png) \n",
    "    \n",
    "Figure 1\n",
    "\n",
    "Generally, the YOLO algorithm has the following steps:\n",
    "\n",
    "1. Divide the image into cells with an **_SxS_** grid\n",
    "2. Each cell predicts **_B_** bounding boxes (_A cell is responsible for detecting an object if the object's bounding box is within the cell_\n",
    "3. Return bounding boxes above a given confidence threshold. _The algorithm will show only the bounding box with the highest probability confidence (e.g. 0.90) and will reject all boxes with less values than this threshold_.\n",
    "\n",
    "**Note:** In practice will like touse larger values of $S and B$, such as $S = 19$ and $B = 5$ to identify more objects, and each cell will output a prediction with a corresponding bounding box for a given image.\n",
    "\n",
    "The below image shows the YOLO algorithm's result, which returns the bounding boxes for the detected objects. For the algorithm to perform efficiently needs to be trained sufficiently because with each iteration (epoch), the detection accuracy increases. Also, the bounding boxes can be in more than one cells without any issue, and the detection is performed in the cell where the midpoint of the bounding box belongs.\n",
    "\n",
    "![image](../notes/images/image2.png)\n",
    "\n",
    "Figure 2\n",
    "\n",
    "The YOLO object detection algorithm is faster architecture because uses one Convolutional Neural Network (CNN) to run all components in the given image in contrast with the naive sliding window approach where for each image the algorithm (DPM, R-CNN etc) needs to scan it step by step to find the region of interest, the detected objects. The R-CNN for example needs classify around 2000 regions per image which makes the algorithm very time consuming and it's not ideal for real time applications.\n",
    "\n",
    "The figure below shows how the YOLO model creates an $S x S$ grid in the input image and then for each grid cell creates multiple bounding boxes as well as class probability map, and at the end gives the final predictions of the objects in the image.\n",
    "\n",
    "![image](../notes/images/yolo_paper.png \"YOLO model image processing\")\n",
    "\n",
    "Figure 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-county",
   "metadata": {},
   "source": [
    "## How the bouning boxes are encoded in YOLO?\n",
    "One of the most important aspects of this algorithm is the it builds and specifies the bounding boxes, and the other is the the Loss function. The algorithm uses five components to predict an output:\n",
    "\n",
    "1. The centre of a bounding box $(b_x b_y)$ relative to the bounds of the grid cell\n",
    "2. The width $(b_w)$\n",
    "3. The height $(b_h)$. The width and the height of the entire image.\n",
    "4. The class of the object $(c)$\n",
    "5. The prediction confidence $(p_c)$ which is the probability of the existance of an object within the bounding box.\n",
    "\n",
    "Thus, we, optimally, want one bounding box for each object in the given image and we can be sure that only one object will be predicted for each object by taking the midpoint of the cell that is responsible for outputing that object.\n",
    "\n",
    "So, each bounding box for each cell will have $[x_1, y_1, x_2, y_2]$ coordinates where in the YOLO algorithm will be $[x, y, w, h]$\n",
    "\n",
    "* $x$ and $y$ will be the coordinates for object midpoint in cell -> these actually will be between $0 - 1$\n",
    "* $w$ and $h$ will be the width and the heigth of that object relative to the cell -> $w$ can be _greater_ than 1, if the object is wider than the cell, and $h$ can also be _greater_ than 1, if the object is taller than the cell\n",
    "\n",
    "The labels will look like the following:\n",
    "\n",
    "$label_{cell} = [c_1, c_2, ..., c_5, p_c, x, y, w,h]$\n",
    "\n",
    "where:\n",
    "\n",
    "* $c_1$ to $c_5$ will be the dataset classes\n",
    "* $p_c$ probability that there is an object (1 or 0)\n",
    "* $x, y, w,h$ are the coordinates of the bounding boxes\n",
    "\n",
    "\n",
    "Predictions will look very similar, but will output two bouning boxes (will specialise to output different bounfding boxes (wide vs tall).\n",
    "\n",
    "$pred_{cell} = [c_1, c_2, ..., c_5, p_{c_1}, x_1, y_1, w_1, h_1, p_{c_2}, x_2, y_2, w_2, h_2]$\n",
    "\n",
    "**Note:** A cell can only detect one object, this is also one of the YOLO limitations (we can have finer grid to achieve multiple detections as mentioned above.\n",
    "\n",
    "This is for every cell and the **target** shape for one image will be $(S, S, 10)$\n",
    "\n",
    "where:\n",
    "\n",
    "* $S * S$ is the grid size\n",
    "* $5$ is for the class predictions, $1$ is for the probability score, and $4$ is for the bouning boxes\n",
    "\n",
    "The **predictions** shape will be $(S, S, 15)$ where there is and additional probability score and four extra bounding box predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-cotton",
   "metadata": {},
   "source": [
    "## The model architecture\n",
    "![image](../notes/images/model.png)\n",
    "\n",
    "The original YOLO model consists of 24 convolutional layers followed by 2 fully connected layers.\n",
    "The model accepts 448x448 images and at the first layer has a 7x7 kernel with 64 output filters with stride of 2 (**also need to have a padding of 3 to much the dimensions**), also there is a 2x2 Maxpool Layer with the stride of 2. Simillarly, the rest of the model consists of convolutional layers and Maxpool layers except the last two layers where there are a fully conected layers where the first one takes as and input the convolutional output and make it a linear layer of 4096 feature vector and outputs to the fully connected which is reshaped to become a 7 by 7 by 30 which is the final split size of the image ($S = 7$ which is a $7$ x $7$ grid) with a vector output of 30 (in my case this will be 15).\n",
    "\n",
    "To help whith the architecture building it will be usefull to pre-determine the architecure configuration:\n",
    "\n",
    "```python\n",
    "architecture_config = [\n",
    "    # Tuple: (kernel_size, num_filters, stride, padding)\n",
    "    (7, 64, 2, 3), \n",
    "    \"M\",    # M stands for the MaxPolling Layer and has stride 2x2 and kernel 2x2\n",
    "    (3, 192, 1, 1),\n",
    "    \"M\",\n",
    "    (1, 128, 1, 0),\n",
    "    (3, 256, 1, 1), \n",
    "    (1, 256, 1, 0), \n",
    "    (3, 512, 1, 1), \n",
    "    \"M\",\n",
    "    # List of tuples: (kernel_size, num_filters, stride, padding), num_of_repeats\n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "    (1, 512, 1, 0), \n",
    "    (3, 1024, 1, 1),\n",
    "    \"M\", \n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2], \n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    (3, 1024, 1, 1), \n",
    "    (3, 1024, 1, 1),\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-iraqi",
   "metadata": {},
   "source": [
    "## The Loss Function\n",
    "\n",
    "The YOLO loss function is the second most important aspect of the algorithm. The basic concept behind all these losses is that are the sum squared error, and if we look at the first part of the loss function is going to be the loss for the box coordinate for the midpoint (taking the $x$ midpoint value and subtractining from the predicted $\\hat{x}$ squared). The $\\mathbb{1}_{ij}^{obj}$ is the identity function which is calculated when there is an object in the cell, so summurizing there is:\n",
    "\n",
    "* $\\mathbb{1}_{i}^{obj}$ is 1 when there is an object in the cell $i$ otherwise is 0.\n",
    "* $\\mathbb{1}_{ij}^{obj}$ is the $j^{th}$ bounding box prediction for the cell $i$ \n",
    "* $\\mathbb{1}_{ij}^{noobj}$ has the same concept with the previous one, except that is 1 when there is no object and 0 when there is an object. \n",
    "\n",
    "So, to know which bounding box is responsible for outputing that bounding box is by looking at the cell and see which of the predicted bounding boxes has the highest Intersection over Union (IoU) value with the target bouning box. The one with the highest IoU will be the responsible bounding box for the prediction and will be send to the loss function. \n",
    "\n",
    "\\begin{align}\n",
    "&\\lambda_{coord} \\sum_{i=0}^{S^2}\\sum_{j=0}^B \\mathbb{1}_{ij}^{obj}[(x_i-\\hat{x}_i)^2 + (y_i-\\hat{y}_i)^2 ] \\\\&+ \\lambda_{coord} \\sum_{i=0}^{S^2}\\sum_{j=0}^B \\mathbb{1}_{ij}^{obj}[(\\sqrt{w_i}-\\sqrt{\\hat{w}_i})^2 +(\\sqrt{h_i}-\\sqrt{\\hat{h}_i})^2 ]\\\\\n",
    "&+ \\sum_{i=0}^{S^2}\\sum_{j=0}^B \\mathbb{1}_{ij}^{obj}(C_i - \\hat{C}_i)^2 + \\lambda_{noobj}\\sum_{i=0}^{S^2}\\sum_{j=0}^B \\mathbb{1}_{ij}^{noobj}(C_i - \\hat{C}_i)^2 \\\\\n",
    "&+ \\sum_{i=0}^{S^2} \\mathbb{1}_{i}^{obj}\\sum_{c \\in classes}(p_i(c) - \\hat{p}_i(c))^2 \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-assets",
   "metadata": {},
   "source": [
    "# Algorithm Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sublime-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.random import seed\n",
    "from torch.functional import chain_matmul \n",
    "from torch.nn.modules import padding\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "handed-respondent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change to yolo dir: /media/ioannis/DATA/Documents/Machine_learning/Project/src/yolo_v1\n"
     ]
    }
   ],
   "source": [
    "# Get the correct path for utils.py script\n",
    "if os.getcwd() == '/media/ioannis/DATA/Documents/Machine_learning/Project/src/yolo_v1':\n",
    "    print(f\"The working direcory is: {os.getcwd()}\")\n",
    "else:    \n",
    "    os.chdir(\"../src/yolo_v1/\")\n",
    "    print(f\"Change to yolo dir: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hungarian-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import intersection_over_union\n",
    "from utils import(\n",
    "    intersection_over_union,\n",
    "    non_max_suppression,\n",
    "    mean_average_precision,\n",
    "    cellboxes_to_boxes,\n",
    "    get_bboxes,\n",
    "    plot_image,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-confirmation",
   "metadata": {},
   "source": [
    "### YOLO model architecure\n",
    "\n",
    "#### Architecture configuration based on YOLO paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sustainable-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_config = [\n",
    "    (7, 64, 2, 3), \n",
    "    \"M\",    # M stands for the MaxPolling Layer and has stride 2x2 and kernel 2x2\n",
    "    (3, 192, 1, 1),\n",
    "    \"M\",\n",
    "    (1, 128, 1, 0),\n",
    "    (3, 256, 1, 1), \n",
    "    (1, 256, 1, 0), \n",
    "    (3, 512, 1, 1), \n",
    "    \"M\", \n",
    "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
    "    (1, 512, 1, 0), \n",
    "    (3, 1024, 1, 1),\n",
    "    \"M\", \n",
    "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2], \n",
    "    (3, 1024, 1, 1),\n",
    "    (3, 1024, 2, 1),\n",
    "    (3, 1024, 1, 1), \n",
    "    (3, 1024, 1, 1),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-texas",
   "metadata": {},
   "source": [
    "#### The YOLO Architecture\n",
    "The CNNBlock class will be used as a block code to build the various convolutional layers in the YoloV1 class, which is the main model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "geographic-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This CNN block is used to as a blueprint of the conv layers for the YoloV1 model.\n",
    "    Need to use convolutional layers multiple times, so we'll use the CNNBlock for easy of use.\n",
    "\n",
    "    Args:\n",
    "        nn ([type]): [description]\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leakyrelu(self.batchnorm(self.conv(x)))\n",
    "        return x\n",
    "    \n",
    "class YoloV1(nn.Module):\n",
    "    def __init__(self, in_channels=3, **kwargs):\n",
    "        super(YoloV1, self).__init__()\n",
    "        self.architecture = architecture_config\n",
    "        self.in_channels = in_channels\n",
    "        self.darknet = self._create_conv_layers(self.architecture)\n",
    "        self.fcs = self._create_fcs(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.darknet(x)\n",
    "        return self.fcs(torch.flatten(x, start_dim=1))\n",
    "\n",
    "    def _create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for x in architecture:\n",
    "            if type(x) == tuple:\n",
    "                layers += [\n",
    "                    CNNBlock(\n",
    "                        in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3],\n",
    "                    )\n",
    "                ]\n",
    "                in_channels = x[1]\n",
    "\n",
    "            elif type(x) == str:\n",
    "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
    "\n",
    "            elif type(x) == list:\n",
    "                conv1 = x[0]\n",
    "                conv2 = x[1]\n",
    "                num_repeats = x[2]\n",
    "\n",
    "                for _ in range(num_repeats):\n",
    "                    layers += [\n",
    "                        CNNBlock(\n",
    "                            in_channels,\n",
    "                            conv1[1],\n",
    "                            kernel_size=conv1[0],\n",
    "                            stride=conv1[2],\n",
    "                            padding=conv1[3],\n",
    "                        )\n",
    "                    ]\n",
    "                    layers += [\n",
    "                        CNNBlock(\n",
    "                            conv1[1],\n",
    "                            conv2[1],\n",
    "                            kernel_size=conv2[0],\n",
    "                            stride=conv2[2],\n",
    "                            padding=conv2[3],\n",
    "                        )\n",
    "                    ]\n",
    "                    in_channels = conv2[1]\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
    "        S, B, C = split_size, num_boxes, num_classes\n",
    "\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * S * S, 496),\n",
    "            nn.Dropout(0.0),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(496, S * S * (C + B * 5)),  # (S, S, 30) where C + B * 5 = 30\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "specialized-default",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 735])\n"
     ]
    }
   ],
   "source": [
    "def test(S=7, B=2, C=5):\n",
    "    \"\"\"\n",
    "    A function to test YoloV1 model\n",
    "    \"\"\"\n",
    "    model = YoloV1(split_size=S, num_boxes=B, num_classes=C)\n",
    "    x = torch.randn((2, 3, 448, 448))\n",
    "    print(model(x).shape)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "pacific-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "         LeakyReLU-3         [-1, 64, 224, 224]               0\n",
      "          CNNBlock-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 192, 112, 112]         110,592\n",
      "       BatchNorm2d-7        [-1, 192, 112, 112]             384\n",
      "         LeakyReLU-8        [-1, 192, 112, 112]               0\n",
      "          CNNBlock-9        [-1, 192, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 192, 56, 56]               0\n",
      "           Conv2d-11          [-1, 128, 56, 56]          24,576\n",
      "      BatchNorm2d-12          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-13          [-1, 128, 56, 56]               0\n",
      "         CNNBlock-14          [-1, 128, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         294,912\n",
      "      BatchNorm2d-16          [-1, 256, 56, 56]             512\n",
      "        LeakyReLU-17          [-1, 256, 56, 56]               0\n",
      "         CNNBlock-18          [-1, 256, 56, 56]               0\n",
      "           Conv2d-19          [-1, 256, 56, 56]          65,536\n",
      "      BatchNorm2d-20          [-1, 256, 56, 56]             512\n",
      "        LeakyReLU-21          [-1, 256, 56, 56]               0\n",
      "         CNNBlock-22          [-1, 256, 56, 56]               0\n",
      "           Conv2d-23          [-1, 512, 56, 56]       1,179,648\n",
      "      BatchNorm2d-24          [-1, 512, 56, 56]           1,024\n",
      "        LeakyReLU-25          [-1, 512, 56, 56]               0\n",
      "         CNNBlock-26          [-1, 512, 56, 56]               0\n",
      "        MaxPool2d-27          [-1, 512, 28, 28]               0\n",
      "           Conv2d-28          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-29          [-1, 256, 28, 28]             512\n",
      "        LeakyReLU-30          [-1, 256, 28, 28]               0\n",
      "         CNNBlock-31          [-1, 256, 28, 28]               0\n",
      "           Conv2d-32          [-1, 512, 28, 28]       1,179,648\n",
      "      BatchNorm2d-33          [-1, 512, 28, 28]           1,024\n",
      "        LeakyReLU-34          [-1, 512, 28, 28]               0\n",
      "         CNNBlock-35          [-1, 512, 28, 28]               0\n",
      "           Conv2d-36          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-37          [-1, 256, 28, 28]             512\n",
      "        LeakyReLU-38          [-1, 256, 28, 28]               0\n",
      "         CNNBlock-39          [-1, 256, 28, 28]               0\n",
      "           Conv2d-40          [-1, 512, 28, 28]       1,179,648\n",
      "      BatchNorm2d-41          [-1, 512, 28, 28]           1,024\n",
      "        LeakyReLU-42          [-1, 512, 28, 28]               0\n",
      "         CNNBlock-43          [-1, 512, 28, 28]               0\n",
      "           Conv2d-44          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-45          [-1, 256, 28, 28]             512\n",
      "        LeakyReLU-46          [-1, 256, 28, 28]               0\n",
      "         CNNBlock-47          [-1, 256, 28, 28]               0\n",
      "           Conv2d-48          [-1, 512, 28, 28]       1,179,648\n",
      "      BatchNorm2d-49          [-1, 512, 28, 28]           1,024\n",
      "        LeakyReLU-50          [-1, 512, 28, 28]               0\n",
      "         CNNBlock-51          [-1, 512, 28, 28]               0\n",
      "           Conv2d-52          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-53          [-1, 256, 28, 28]             512\n",
      "        LeakyReLU-54          [-1, 256, 28, 28]               0\n",
      "         CNNBlock-55          [-1, 256, 28, 28]               0\n",
      "           Conv2d-56          [-1, 512, 28, 28]       1,179,648\n",
      "      BatchNorm2d-57          [-1, 512, 28, 28]           1,024\n",
      "        LeakyReLU-58          [-1, 512, 28, 28]               0\n",
      "         CNNBlock-59          [-1, 512, 28, 28]               0\n",
      "           Conv2d-60          [-1, 512, 28, 28]         262,144\n",
      "      BatchNorm2d-61          [-1, 512, 28, 28]           1,024\n",
      "        LeakyReLU-62          [-1, 512, 28, 28]               0\n",
      "         CNNBlock-63          [-1, 512, 28, 28]               0\n",
      "           Conv2d-64         [-1, 1024, 28, 28]       4,718,592\n",
      "      BatchNorm2d-65         [-1, 1024, 28, 28]           2,048\n",
      "        LeakyReLU-66         [-1, 1024, 28, 28]               0\n",
      "         CNNBlock-67         [-1, 1024, 28, 28]               0\n",
      "        MaxPool2d-68         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-69          [-1, 512, 14, 14]         524,288\n",
      "      BatchNorm2d-70          [-1, 512, 14, 14]           1,024\n",
      "        LeakyReLU-71          [-1, 512, 14, 14]               0\n",
      "         CNNBlock-72          [-1, 512, 14, 14]               0\n",
      "           Conv2d-73         [-1, 1024, 14, 14]       4,718,592\n",
      "      BatchNorm2d-74         [-1, 1024, 14, 14]           2,048\n",
      "        LeakyReLU-75         [-1, 1024, 14, 14]               0\n",
      "         CNNBlock-76         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-77          [-1, 512, 14, 14]         524,288\n",
      "      BatchNorm2d-78          [-1, 512, 14, 14]           1,024\n",
      "        LeakyReLU-79          [-1, 512, 14, 14]               0\n",
      "         CNNBlock-80          [-1, 512, 14, 14]               0\n",
      "           Conv2d-81         [-1, 1024, 14, 14]       4,718,592\n",
      "      BatchNorm2d-82         [-1, 1024, 14, 14]           2,048\n",
      "        LeakyReLU-83         [-1, 1024, 14, 14]               0\n",
      "         CNNBlock-84         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]       9,437,184\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "        LeakyReLU-87         [-1, 1024, 14, 14]               0\n",
      "         CNNBlock-88         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-89           [-1, 1024, 7, 7]       9,437,184\n",
      "      BatchNorm2d-90           [-1, 1024, 7, 7]           2,048\n",
      "        LeakyReLU-91           [-1, 1024, 7, 7]               0\n",
      "         CNNBlock-92           [-1, 1024, 7, 7]               0\n",
      "           Conv2d-93           [-1, 1024, 7, 7]       9,437,184\n",
      "      BatchNorm2d-94           [-1, 1024, 7, 7]           2,048\n",
      "        LeakyReLU-95           [-1, 1024, 7, 7]               0\n",
      "         CNNBlock-96           [-1, 1024, 7, 7]               0\n",
      "           Conv2d-97           [-1, 1024, 7, 7]       9,437,184\n",
      "      BatchNorm2d-98           [-1, 1024, 7, 7]           2,048\n",
      "        LeakyReLU-99           [-1, 1024, 7, 7]               0\n",
      "        CNNBlock-100           [-1, 1024, 7, 7]               0\n",
      "         Flatten-101                [-1, 50176]               0\n",
      "          Linear-102                  [-1, 496]      24,887,792\n",
      "         Dropout-103                  [-1, 496]               0\n",
      "       LeakyReLU-104                  [-1, 496]               0\n",
      "          Linear-105                  [-1, 735]         365,295\n",
      "================================================================\n",
      "Total params: 85,422,239\n",
      "Trainable params: 85,422,239\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.30\n",
      "Forward/backward pass size (MB): 436.81\n",
      "Params size (MB): 325.86\n",
      "Estimated Total Size (MB): 764.96\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# YOLO V1 model summary\n",
    "\n",
    "from torchsummary import summary\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "S=7\n",
    "B=2 \n",
    "C=5\n",
    "model = YoloV1(split_size=S, num_boxes=B, num_classes=C).to(DEVICE)\n",
    "summary(model, (3, 448, 448))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-springer",
   "metadata": {},
   "source": [
    "So, if we manually calculate the tensor shape we will get:\n",
    "\n",
    "\\begin{align}\n",
    "S * S * (B * 5 + C) \n",
    "=> 7 * 7 * (2 * 5 + 5) = 245 * 3 = 735\n",
    "\\end{align}\n",
    "\n",
    "**Note:** 3 is the number of channels in the photo (RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-session",
   "metadata": {},
   "source": [
    "### Code implementation of Yolo loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sound-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLoss(nn.Module):\n",
    "    def __init__(self, S=7, B=2, C=5):\n",
    "        super(YoloLoss, self).__init__()\n",
    "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "        self.lambda_noobj = 0.5\n",
    "        self.lambda_coord = 5\n",
    "\n",
    "    def forward(self, predictions, target):\n",
    "        predictions = predictions.reshape(\n",
    "            -1,  self.S, self.S, self.C + self.B * 5\n",
    "            )\n",
    "        iou_b1 = intersection_over_union(predictions[..., 6:10], target[..., 6:10])\n",
    "        iou_b2 = intersection_over_union(predictions[..., 11:15], target[..., 6:10])\n",
    "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
    "\n",
    "        iou_maxes, best_box = torch.max(ious, dim=0)\n",
    "        exists_box = target[..., 5].unsqueeze(3)  # Iobj_i identity_of_object_i\n",
    "\n",
    "        # For Box Coordinates\n",
    "        box_predictions = exists_box * (\n",
    "            (\n",
    "                best_box * predictions[..., 11:15]\n",
    "                + (1 - best_box) * predictions[..., 6:10]\n",
    "             )\n",
    "        )\n",
    "        box_targets = exists_box * target[..., 6:10]\n",
    "\n",
    "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(\n",
    "            torch.abs(box_predictions[..., 2:4] + 1e-6))\n",
    "\n",
    "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
    "\n",
    "        box_loss = self.mse(\n",
    "            torch.flatten(box_predictions, end_dim=-2),\n",
    "            torch.flatten(box_targets, end_dim=-2),\n",
    "        )\n",
    "\n",
    "        # For Object Loss\n",
    "        pred_box = (\n",
    "            best_box *\n",
    "            predictions[..., 10:11] + (1 - best_box) *\n",
    "            predictions[..., 5:6]\n",
    "        )\n",
    "\n",
    "        object_loss = self.mse(\n",
    "            torch.flatten(exists_box * pred_box),\n",
    "            torch.flatten(exists_box * target[..., 5:6])\n",
    "        )\n",
    "\n",
    "        # For no Object Loss\n",
    "        # (N, S, S, 1) -> (N, S*S)\n",
    "        no_object_loss = self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 5:6], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 5:6], start_dim=1)\n",
    "        )\n",
    "\n",
    "        no_object_loss += self.mse(\n",
    "            torch.flatten((1 - exists_box) * predictions[..., 10:11], start_dim=1),\n",
    "            torch.flatten((1 - exists_box) * target[..., 5:6], start_dim=1)\n",
    "        )\n",
    "\n",
    "        # For Class Loss\n",
    "        # (N, S, S, 20) -> (N*S*S, 20)\n",
    "        class_loss = self.mse(\n",
    "            torch.flatten(exists_box * predictions[..., :5], end_dim=-2,),\n",
    "            torch.flatten(exists_box * target[..., :5], end_dim=-2,),\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            self.lambda_coord * box_loss    # First two rows of loss in paper\n",
    "            + object_loss\n",
    "            + self.lambda_noobj * no_object_loss\n",
    "            + class_loss\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-aspect",
   "metadata": {},
   "source": [
    "### Dataset Class for custom dataset from Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "furnished-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self, csv_file, img_dir, label_dir, S=7, B=2, C=5, transform=None\n",
    "    ):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.S = S\n",
    "        self.B = B\n",
    "        self.C = C\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_path = os.path.join(self.label_dir,\n",
    "                                  self.annotations.iloc[index, 1])\n",
    "        boxes = []\n",
    "        # read the labels in the yolo annotations and append to boxes list\n",
    "        with open(label_path) as f:\n",
    "            for label in f.readlines():\n",
    "                # print(label)\n",
    "                class_label, x, y, width, height = [\n",
    "                    x for x in label.replace(\"\\n\", \" \").split()\n",
    "                ]\n",
    "\n",
    "                boxes.append([int(class_label), float(x), float(y), float(width), float(height)])\n",
    "        # print(len(boxes))\n",
    "        # read the images of the dataset\n",
    "        img_path = os.path.join(self.img_dir,\n",
    "                                self.annotations.iloc[index, 0])\n",
    "        image = Image.open(img_path)\n",
    "        boxes = torch.tensor(boxes)\n",
    "\n",
    "        if self.transform:\n",
    "            image, boxes = self.transform(image, boxes)\n",
    "\n",
    "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
    "\n",
    "        for box in boxes:\n",
    "            class_label, x, y, width, height = box.tolist()\n",
    "            class_label = int(class_label)\n",
    "            i, j = int(self.S * y), int(self.S * x)\n",
    "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
    "\n",
    "            width_cell, height_cell = (\n",
    "                width * self.S,\n",
    "                height * self.S,\n",
    "            )\n",
    "\n",
    "            if label_matrix[i, j, 5] == 0:\n",
    "                label_matrix[i, j, 5] = 1\n",
    "\n",
    "                box_coordinates = torch.tensor(\n",
    "                    [x_cell, y_cell, width_cell, height_cell]\n",
    "                )\n",
    "\n",
    "                label_matrix[i, j, 6:10] = box_coordinates\n",
    "\n",
    "                # Set one hot encoding for class_labels\n",
    "                label_matrix[i, j, class_label] = 1\n",
    "\n",
    "        return image, label_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "colored-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the logging module to create a training log file\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "file_Handler = logging.FileHandler(f'{time.strftime(\"%Y_%m_%d\")}_training.log', mode=\"w\")\n",
    "logger.addHandler(file_Handler)\n",
    "logger.info(\"******\" * 20)\n",
    "\n",
    "# to get the same dataset loading each time \n",
    "seed = 123\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# Setup the Hyperparameters\n",
    "\n",
    "LEARNING_RATE = 2E-5\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "BATCH_SIZE = 4\n",
    "WEIGHT_DECAY = 0\n",
    "EPOCHS = 100\n",
    "NUM_WORKERS = 4\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "LOAD_MODEL_FILE = \"model_training/overfit.pth.tar\"\n",
    "IMG_DIR = \"dataset/images\"\n",
    "LABEL_DIR = \"dataset/labels\"\n",
    "\n",
    "\n",
    "class Compose(object):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, bboxes):\n",
    "        for t in self.transforms:\n",
    "            img, bboxes = t(img), bboxes\n",
    "        \n",
    "        return img, bboxes\n",
    "\n",
    "\n",
    "transform = Compose([\n",
    "    transforms.Resize((448, 448)),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "\n",
    "def train_fn(train_loader, model, optimizer, loss_fn):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    mean_loss = []\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        loss = loss_fn(out, y)\n",
    "        mean_loss.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "    \n",
    "    print(f\"Mean loss was {sum(mean_loss) / len(mean_loss)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    model = YoloV1(split_size=7, num_boxes=2, num_classes=5).to(DEVICE)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    loss_fn = YoloLoss()\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        load_checkpoint(torch.load(LOAD_MODEL_FILE), model, optimizer)  \n",
    "\n",
    "    train_dataset = LabDataset(\n",
    "        \"dataset/train.csv\",\n",
    "        transform=transform,\n",
    "        img_dir=IMG_DIR,\n",
    "        label_dir=LABEL_DIR,\n",
    "    )\n",
    "\n",
    "    test_dataset = LabDataset(\n",
    "        \"dataset/test.csv\",\n",
    "        transform=transform,\n",
    "        img_dir=IMG_DIR,\n",
    "        label_dir=LABEL_DIR,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=PIN_MEMORY,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # for x, y in train_loader:\n",
    "        #     x = x.to(DEVICE)\n",
    "        #     for idx in range(8):\n",
    "        #         bboxes = cellboxes_to_boxes(model(x))\n",
    "        #         bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "        #         plot_image(x[idx].permute(1,2,0).to(\"cpu\"), bboxes)\n",
    "        # import sys\n",
    "        # sys.exit()\n",
    "\n",
    "        pred_boxes, target_boxes = get_bboxes(\n",
    "            train_loader, model, iou_threshold=0.5, threshold=0.4\n",
    "        )\n",
    "        mean_average_prec = mean_average_precision(\n",
    "            pred_boxes, target_boxes, iou_threshold=0.5, box_format=\"midpoint\"\n",
    "        )\n",
    "\n",
    "        print(f\"Train mAP: {mean_average_prec}\")\n",
    "\n",
    "        # if mean_average_prec > 0.9:\n",
    "        #     checkpoint = {\n",
    "        #         \"state_dict\": model.state_dict(),\n",
    "        #         \"optimizer\": optimizer.state_dict(),\n",
    "        #     }\n",
    "        #     save_checkpoint(checkpoint, filename=LOAD_MODEL_FILE)\n",
    "        #     import time\n",
    "        #     time.sleep(10)\n",
    "\n",
    "        train_fn(train_loader, model, optimizer, loss_fn)\n",
    "\n",
    "    for x, y in train_loader:\n",
    "        x = x.to(DEVICE)\n",
    "        # train_len = open(\"dataset/train.csv\", \"r\").readlines()\n",
    "        for idx in range(4):\n",
    "            bboxes = cellboxes_to_boxes(model(x))\n",
    "            bboxes = non_max_suppression(bboxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
    "            plot_image(x[idx].permute(1, 2, 0).to(\"cpu\"), bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rolled-willow",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-shade",
   "metadata": {},
   "source": [
    "### Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-boston",
   "metadata": {},
   "source": [
    "#### Training mean Average Precision\n",
    "\n",
    "```bash\n",
    "torch.Size([2, 735])\n",
    "Train mAP: 0.0\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.53it/s, loss=230]\n",
    "Mean loss was 277.01525966937726\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 1.5975972473825095e-06\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.73it/s, loss=125]\n",
    "Mean loss was 150.3601137307974\n",
    "Train mAP: 0.00036260823253542185\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.59it/s, loss=131]\n",
    "Mean loss was 111.0760328586285\n",
    "Train mAP: 0.0010389338713139296\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.56it/s, loss=67.1]\n",
    "Mean loss was 93.82369613647461\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.0017443771939724684\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.35it/s, loss=101]\n",
    "Mean loss was 85.7502593260545\n",
    "Train mAP: 0.0019065936794504523\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.59it/s, loss=73.2]\n",
    "Mean loss was 79.8317461013794\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.0021070390939712524\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.66it/s, loss=42.9]\n",
    "Mean loss was 74.16719363285945\n",
    "Train mAP: 0.008167828433215618\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.66it/s, loss=71]\n",
    "Mean loss was 68.89766018207257\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.01164398156106472\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.49it/s, loss=59.6]\n",
    "Mean loss was 65.96115589141846\n",
    "Train mAP: 0.022016068920493126\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.59it/s, loss=60.7]\n",
    "Mean loss was 63.24265480041504\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.026762772351503372\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.63it/s, loss=49.2]\n",
    "Mean loss was 59.42209089719332\n",
    "Train mAP: 0.03686966747045517\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.46it/s, loss=70.4]\n",
    "Mean loss was 57.88046492063082\n",
    "Train mAP: 0.04176099970936775\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.50it/s, loss=69.4]\n",
    "Mean loss was 53.74426863743709\n",
    "Train mAP: 0.06215560436248779\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.48it/s, loss=47.8]\n",
    "Mean loss was 53.232022505540115\n",
    "Train mAP: 0.04161568731069565\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.63it/s, loss=66.5]\n",
    "Mean loss was 51.478044509887695\n",
    "Train mAP: 0.05706929415464401\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.64it/s, loss=71.5]\n",
    "Mean loss was 49.621550193199745\n",
    "Train mAP: 0.09943243861198425\n",
    "100%|██████████| 52/52 [00:07<00:00,  6.60it/s, loss=39.7]\n",
    "Mean loss was 46.71747504747831\n",
    "Train mAP: 0.09798908233642578\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.59it/s, loss=37.2]\n",
    "Mean loss was 44.66095953721266\n",
    "Train mAP: 0.09688045084476471\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.17it/s, loss=39.6]\n",
    "Mean loss was 46.553726746485786\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.10760778188705444\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.56it/s, loss=48.2]\n",
    "Mean loss was 45.06020501943735\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.13196484744548798\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.27it/s, loss=29.7]\n",
    "Mean loss was 42.605860196627106\n",
    "Train mAP: 0.12919731438159943\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.09it/s, loss=41]\n",
    "Mean loss was 43.64498409858117\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.12473483383655548\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.26it/s, loss=37.6]\n",
    "Mean loss was 42.67212941096379\n",
    "Train mAP: 0.15012900531291962\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.35it/s, loss=40.9]\n",
    "Mean loss was 41.00279015761156\n",
    "Train mAP: 0.18024876713752747\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.01it/s, loss=36.3]\n",
    "Mean loss was 41.815959233504074\n",
    "Train mAP: 0.19966743886470795\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.22it/s, loss=33.7]\n",
    "Mean loss was 41.42074548281156\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.14009423553943634\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.42it/s, loss=38.7]\n",
    "Mean loss was 40.43391271737906\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.20363807678222656\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.23it/s, loss=25.4]\n",
    "Mean loss was 36.65179105905386\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.23210477828979492\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.42it/s, loss=35.6]\n",
    "Mean loss was 36.59433174133301\n",
    "Train mAP: 0.20036259293556213\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.26it/s, loss=45.4]\n",
    "Mean loss was 37.1617332972013\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.24710893630981445\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.44it/s, loss=59.9]\n",
    "Mean loss was 36.56079688439002\n",
    "Train mAP: 0.252134770154953\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.52it/s, loss=23.9]\n",
    "Mean loss was 34.84693479537964\n",
    "Train mAP: 0.3164646029472351\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.13it/s, loss=27.4]\n",
    "Mean loss was 32.70728085591243\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.27682849764823914\n",
    "100%|██████████| 52/52 [00:07<00:00,  6.77it/s, loss=22.3]\n",
    "Mean loss was 33.125311888181244\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.31891489028930664\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.52it/s, loss=37.4]\n",
    "Mean loss was 31.009997001061073\n",
    "Train mAP: 0.2684682011604309\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.38it/s, loss=26.1]\n",
    "Mean loss was 29.65498792208158\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.31201815605163574\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.47it/s, loss=26.2]\n",
    "Mean loss was 29.540525986598087\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.2407427281141281\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.48it/s, loss=35.6]\n",
    "Mean loss was 33.021992500011734\n",
    "Train mAP: 0.3038192093372345\n",
    "100%|██████████| 52/52 [00:07<00:00,  6.93it/s, loss=25.7]\n",
    "Mean loss was 31.74266558427077\n",
    "Train mAP: 0.3210943639278412\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.58it/s, loss=26.2]\n",
    "Mean loss was 29.95376906028161\n",
    "Train mAP: 0.35638627409935\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.20it/s, loss=34.3]\n",
    "Mean loss was 27.2260375389686\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.33736562728881836\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.14it/s, loss=17.3]\n",
    "Mean loss was 28.47225623864394\n",
    "Train mAP: 0.3314797282218933\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.34it/s, loss=21.2]\n",
    "Mean loss was 27.52292165389428\n",
    "Train mAP: 0.38283103704452515\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.60it/s, loss=29.2]\n",
    "Mean loss was 26.119591272794285\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.41640034317970276\n",
    "100%|██████████| 52/52 [00:07<00:00,  6.87it/s, loss=33.2]\n",
    "Mean loss was 29.60625433921814\n",
    "Train mAP: 0.25621533393859863\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.52it/s, loss=30.1]\n",
    "Mean loss was 32.28092266963078\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.3336578905582428\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.30it/s, loss=18.4]\n",
    "Mean loss was 26.53524373127864\n",
    "Train mAP: 0.3782862722873688\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.60it/s, loss=20.1]\n",
    "Mean loss was 24.900437575120193\n",
    "Train mAP: 0.38781529664993286\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.16it/s, loss=30.8]\n",
    "Mean loss was 23.54118515894963\n",
    "Train mAP: 0.39370396733283997\n",
    "100%|██████████| 52/52 [00:07<00:00,  6.68it/s, loss=23.4]\n",
    "Mean loss was 22.866925588020912\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.41854867339134216\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.42it/s, loss=14.1]\n",
    "Mean loss was 23.564331861642692\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.38859686255455017\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.30it/s, loss=21.6]\n",
    "Mean loss was 23.251168507796066\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.4097711145877838\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.24it/s, loss=25.2]\n",
    "Mean loss was 22.88079210428091\n",
    "Train mAP: 0.48224368691444397\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.68it/s, loss=30.7]\n",
    "Mean loss was 30.80921220779419\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.32274675369262695\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.25it/s, loss=24.8]\n",
    "Mean loss was 28.22428868367122\n",
    "Train mAP: 0.3558574318885803\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.35it/s, loss=21.9]\n",
    "Mean loss was 26.428694394918587\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.3741680681705475\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.47it/s, loss=25.3]\n",
    "Mean loss was 26.91001536295964\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.41832470893859863\n",
    "100%|██████████| 52/52 [00:06<00:00,  7.58it/s, loss=16.7]\n",
    "Mean loss was 23.46789888235239\n",
    "Train mAP: 0.4454154372215271\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.31it/s, loss=21.8]\n",
    "Mean loss was 19.80866953042837\n",
    "Train mAP: 0.5059086084365845\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.14it/s, loss=24]\n",
    "Mean loss was 17.913883154208843\n",
    "Train mAP: 0.5745978355407715\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.35it/s, loss=13.1]\n",
    "Mean loss was 17.709768258608303\n",
    "Train mAP: 0.5389236211776733\n",
    "100%|██████████| 52/52 [00:07<00:00,  6.70it/s, loss=17.3]\n",
    "Mean loss was 18.873461264830368\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.4622669219970703\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.14it/s, loss=22.2]\n",
    "Mean loss was 17.93864067701193\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.4740144610404968\n",
    "100%|██████████| 52/52 [00:07<00:00,  6.87it/s, loss=15.5]\n",
    "Mean loss was 17.754800374691303\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.47399455308914185\n",
    "100%|██████████| 52/52 [00:07<00:00,  6.78it/s, loss=17.3]\n",
    "Mean loss was 18.328510302763718\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.5266884565353394\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.18it/s, loss=16.2]\n",
    "Mean loss was 19.317802851016705\n",
    "Train mAP: 0.5007422566413879\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.07it/s, loss=17.1]\n",
    "Mean loss was 18.273959544988777\n",
    "Train mAP: 0.47520798444747925\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.07it/s, loss=13.7]\n",
    "Mean loss was 18.052292035176205\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.5374414920806885\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.09it/s, loss=11.7]\n",
    "Mean loss was 18.70733952522278\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.3883427679538727\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.14it/s, loss=14.2]\n",
    "Mean loss was 18.25417786378127\n",
    "Train mAP: 0.5039719939231873\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.23it/s, loss=10.5]\n",
    "Mean loss was 15.796511576725887\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.5351071357727051\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.17it/s, loss=14.7]\n",
    "Mean loss was 16.732610097298256\n",
    "Train mAP: 0.45163068175315857\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.28it/s, loss=10.7]\n",
    "Mean loss was 19.227208871107834\n",
    "Train mAP: 0.45331859588623047\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.28it/s, loss=11.6]\n",
    "Mean loss was 17.978595880361702\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.5259262919425964\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.01it/s, loss=26.4]\n",
    "Mean loss was 20.94123187431922\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.4709042012691498\n",
    "100%|██████████| 52/52 [00:07<00:00,  6.90it/s, loss=14.5]\n",
    "Mean loss was 30.559716609808113\n",
    "Train mAP: 0.43308621644973755\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.35it/s, loss=13.3]\n",
    "Mean loss was 26.60452835376446\n",
    "Train mAP: 0.41244402527809143\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.01it/s, loss=19]\n",
    "Mean loss was 21.87913021674523\n",
    "Train mAP: 0.4528992772102356\n",
    "100%|██████████| 52/52 [00:07<00:00,  6.90it/s, loss=9.83]\n",
    "Mean loss was 18.8553817822383\n",
    "Train mAP: 0.481926828622818\n",
    "100%|██████████| 52/52 [00:07<00:00,  6.57it/s, loss=26.3]\n",
    "Mean loss was 16.028261413941017\n",
    "Train mAP: 0.5580487847328186\n",
    "100%|██████████| 52/52 [00:07<00:00,  6.71it/s, loss=12.4]\n",
    "Mean loss was 15.23406903560345\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.5300213694572449\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.27it/s, loss=10.8]\n",
    "Mean loss was 13.7139373926016\n",
    "Train mAP: 0.5464785099029541\n",
    "100%|██████████| 52/52 [00:07<00:00,  6.68it/s, loss=11.6]\n",
    "Mean loss was 14.1117734175462\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.579340934753418\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.08it/s, loss=12.6]\n",
    "Mean loss was 15.100121534787691\n",
    "Train mAP: 0.5814196467399597\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.15it/s, loss=11.3]\n",
    "Mean loss was 16.984538490955646\n",
    "Train mAP: 0.5180677771568298\n",
    "100%|██████████| 52/52 [00:07<00:00,  6.94it/s, loss=10]\n",
    "Mean loss was 16.059333617870625\n",
    "Train mAP: 0.5292803049087524\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.15it/s, loss=18.4]\n",
    "Mean loss was 13.16787363932683\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.586297869682312\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.27it/s, loss=8.94]\n",
    "Mean loss was 12.104050205304073\n",
    "Train mAP: 0.5984959602355957\n",
    "100%|██████████| 52/52 [00:07<00:00,  6.96it/s, loss=8.55]\n",
    "Mean loss was 11.733054436170137\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.5529853701591492\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.02it/s, loss=9.72]\n",
    "Mean loss was 11.902309912901659\n",
    "Train mAP: 0.5623416304588318\n",
    "100%|██████████| 52/52 [00:07<00:00,  6.95it/s, loss=6.27]\n",
    "Mean loss was 12.17488286128411\n",
    "Train mAP: 0.5263509154319763\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.16it/s, loss=20.2]\n",
    "Mean loss was 11.95502089537107\n",
    "Train mAP: 0.5501671433448792\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.16it/s, loss=8.3]\n",
    "Mean loss was 11.710243546045744\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.583227813243866\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.02it/s, loss=26.3]\n",
    "Mean loss was 12.27673394863422\n",
    "Train mAP: 0.5810926556587219\n",
    "100%|██████████| 52/52 [00:07<00:00,  6.90it/s, loss=13.2]\n",
    "Mean loss was 11.398397807891552\n",
    "Train mAP: 0.5858425498008728\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.23it/s, loss=11]\n",
    "Mean loss was 9.952778064287626\n",
    "Train mAP: 0.6241357922554016\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.12it/s, loss=15.2]\n",
    "Mean loss was 11.054220538872938\n",
    "Train mAP: 0.5691715478897095\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.10it/s, loss=12.4]\n",
    "Mean loss was 14.413500584088839\n",
    "Train mAP: 0.5697323083877563\n",
    "100%|██████████| 52/52 [00:07<00:00,  6.99it/s, loss=9.85]\n",
    "Mean loss was 17.656728909565853\n",
    "  0%|          | 0/52 [00:00<?, ?it/s]\n",
    "Train mAP: 0.4619218707084656\n",
    "100%|██████████| 52/52 [00:07<00:00,  7.17it/s, loss=19.4]\n",
    "Mean loss was 16.712135039843044\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-cloud",
   "metadata": {},
   "source": [
    "#### Object bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-satin",
   "metadata": {},
   "source": [
    "![](../notes/images/Figure_1.png)\n",
    "![](../notes/images/Figure_2.png)\n",
    "![](../notes/images/Figure_3.png)\n",
    "![](../notes/images/Figure_4.png)\n",
    "![](../notes/images/Figure_5.png)\n",
    "![](../notes/images/Figure_6.png)\n",
    "![](../notes/images/Figure_7.png)\n",
    "![](../notes/images/Figure_8.png)\n",
    "![](../notes/images/Figure_9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "surface-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot, make_dot_from_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "israeli-relaxation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Digraph.gv.pdf'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YoloV1(split_size=7, num_boxes=2, num_classes=5)\n",
    "\n",
    "x = torch.randn((2, 3, 448, 448)).requires_grad_(True)\n",
    "y = model(x)\n",
    "vis_graph = make_dot(y, params=dict(list(model.named_parameters()) + [('x', x)]))   #.render(\"attached\", format=\"png\")\n",
    "\n",
    "vis_graph.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ignored-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('runs/test')\n",
    "writer.add_graph(model, torch.zeros([2, 3, 448, 448]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "centered-gnome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 381560), started 0:02:20 ago. (Use '!kill 381560' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c603240911f2e624\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c603240911f2e624\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-effects",
   "metadata": {},
   "source": [
    "# Use the Albumentations library for image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "rental-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "\n",
    "image_path = \"dataset/images/images_0.jpg\"\n",
    "transform = A.Compose([\n",
    "    A.RandomCrop(width=256, height=256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "])\n",
    "\n",
    "# Read an image with OpenCV and convert it to the RGB colorspace\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Augment an image\n",
    "transformed = transform(image=image)\n",
    "transformed_image = transformed[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cellular-genetics",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot_image() missing 1 required positional argument: 'boxes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-b256d706e110>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: plot_image() missing 1 required positional argument: 'boxes'"
     ]
    }
   ],
   "source": [
    "plot_image(transformed_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-contrary",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 (torch-gpu)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
